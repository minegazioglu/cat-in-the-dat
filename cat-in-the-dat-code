#importing necessary packages

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.svm import SVC
import seaborn as sns
import datetime as dt
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
import string
import math


#importing train & test data

train = pd.read_csv("Kaggle/catinthedat/train.csv")
test = pd.read_csv("Kaggle/catinthedat/test.csv")

#dictionary for encoding bin_3 & bin_4

bin34_dict = {"T":1,"F":0,"Y":1,"N":0}

# Encoding bin_3 and bin_4

train["bin_3"] = train["bin_3"].map(bin34_dict)
train["bin_4"] = train["bin_4"].map(bin34_dict)


test["bin_3"] = test["bin_3"].map(bin34_dict)
test["bin_4"] = test["bin_4"].map(bin34_dict)

# Using RGB Color System -- map the numbers to the column then use OneHotEncoding [nom_0]
# Red -> (255,0,0)
# Blue -> (0,0,255)
# Green -> (0,128,0)

#nom_0_dict = {"Red":255,"Blue":255,"Green":128}
#train["nom_0"] = train["nom_0"].map(nom_0_dict)
train = pd.get_dummies(train, columns = ["nom_0"])
test = pd.get_dummies(test, columns = ["nom_0"])

nom_0_Blue_dict = {0:0,1:255}
nom_0_Green_dict = {0:0,1:128}
nom_0_Red_dict = {0:0,1:255}
train["nom_0_Blue"] = train["nom_0_Blue"].map(nom_0_Blue_dict)
train["nom_0_Green"] = train["nom_0_Green"].map(nom_0_Green_dict)
train["nom_0_Red"] = train["nom_0_Red"].map(nom_0_Red_dict)

test["nom_0_Blue"] = test["nom_0_Blue"].map(nom_0_Blue_dict)
test["nom_0_Green"] = test["nom_0_Green"].map(nom_0_Green_dict)
test["nom_0_Red"] = test["nom_0_Red"].map(nom_0_Red_dict)


# Ord_1

ord1_dict = {"Novice":1,"Contributor":2,"Expert":3,"Master":4,"Grandmaster":5}
train["ord_1"] = train["ord_1"].map(ord1_dict)

ord1_dict = {"Novice":1,"Contributor":2,"Expert":3,"Master":4,"Grandmaster":5}
test["ord_1"] = test["ord_1"].map(ord1_dict)

# Ord_2

ord2_dict = {"Freezing":1,"Cold":2,"Warm":3,"Hot":4,"Boiling Hot":5,"Lava Hot":6}
train["ord_2"] = train["ord_2"].map(ord2_dict)

ord2_dict = {"Freezing":1,"Cold":2,"Warm":3,"Hot":4,"Boiling Hot":5,"Lava Hot":6}
test["ord_2"] = test["ord_2"].map(ord2_dict)

# Month sin-cosin transformation

train['mnth_sin'] = np.sin((train.month-1)*(2.*np.pi/12.0))
train['mnth_cos'] = np.cos((train.month-1)*(2.*np.pi/12.0))


test['mnth_sin'] = np.sin((test.month-1)*(2.*np.pi/12.0))
test['mnth_cos'] = np.cos((test.month-1)*(2.*np.pi/12.0))

# Day sin-cosin transformation

train['day_sin'] = np.sin((train.day-1)*(2.*np.pi/6.0))
train['day_cos'] = np.cos((train.day-1)*(2.*np.pi/6.0))


test['day_sin'] = np.sin((test.day-1)*(2.*np.pi/6.0))
test['day_cos'] = np.cos((test.day-1)*(2.*np.pi/6.0))


# Stratified KFold

new_train = train.drop(["id","target","day","month"], axis = 1)
new_test = test.drop(["id"], axis = 1, inplace = True)


X = new_train
y = train["target"]


from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler

skf = StratifiedKFold(n_splits=10)

scores = []
i = 1
for train_index, test_index in skf.split(X, y):
    print('{} of KFold {}'.format(i,skf.n_splits))
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    X_test["id"] = X_test.index
    X_train["id"] = X_train.index
    basic_train = pd.concat([X_train,y_train], axis = 1)
    ## Target Encoding
    columns_p = ['nom_1', 'nom_2','nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9','ord_3', 'ord_4', 'ord_5']
    for column in columns_p:
        df_dict = {}
        basic_df = basic_train.groupby([column,"target"]).count()["id"].reset_index()      
        for i,element in enumerate(basic_df[column]):
            total_1 = basic_df[(basic_df[column] == element) & (basic_df["target"] == 1)]["id"].values
            total = (basic_df[(basic_df[column] == element) & (basic_df["target"] == 1)]["id"].values) + (basic_df[(basic_df[column] == element) & (basic_df["target"] == 0)]["id"].values)
            total_0 = (basic_df[(basic_df[column] == element) & (basic_df["target"] == 0)]["id"].values)
            if np.all((len(total_1) != 0) & (len(total_0) != 0)):
                df_dict[element] = (total_1.item() / total.item())
            elif len(total_0) == 0 :
                df_dict[element] = 1   
            else:
                df_dict[element] = 0
    

        X_train[column] = X_train[column].map(df_dict)
        X_test[column] = X_test[column].map(df_dict)
    
    
    X_train.drop(["id"], axis = 1, inplace = True)
    X_test.drop(["id"], axis = 1, inplace = True)
    X_train.fillna(X_train.mean(),inplace = True)
    X_test.fillna(X_train.mean(),inplace = True)
    #Grid Search
    
    #logreg = LogisticRegression()
    #param = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1,2,3,3,4,5,10,20]}
    #clf = GridSearchCV(logreg,param,scoring='roc_auc',cv=10)
    #clf.fit(X_train,y_train)
    #print('Best roc_auc: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_))
    #LogisticRegressionCV
    
    lrcv = LogisticRegressionCV(cv=10)
    lrcv.fit(X_train,y_train)
    predictions = lrcv.predict_proba(X_test)[:2, :]
    #LogisticRegression
    
    #lr = LogisticRegression(penalty = "l1",solver = "liblinear",C=0.01 ,random_state = 42)
    #lr.fit(X_train,y_train)
    #predictions = lr.predict_proba(X_test)[:,1]
    try:
        score = roc_auc_score(y_test,predictions)
    except ValueError:
        pass
    scores.append(score)
    print('ROC AUC score:',score)
    i+=1
    
final_score = sum(scores) / len(scores)
    
